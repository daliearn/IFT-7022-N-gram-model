{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "import numpy as np\n",
    "import os\n",
    "import codecs\n",
    "positivesFiles = os.listdir('./books/Book/pos_Bk/')\n",
    "negativesFiles = os.listdir('./books/Book/neg_Bk/')\n",
    "\n",
    "#Loading Data\n",
    "positives = list(map(lambda f: codecs.open('./books/Book/pos_Bk/'+f, 'r', encoding='latin1').read(), positivesFiles))\n",
    "negatives = list(map(lambda f: codecs.open('./books/Book/neg_Bk/'+f, 'r', encoding='latin1').read(), negativesFiles))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score on training set : 0.964\n",
      "score on 10 Cross Val with 10 folds : 0.7545000000000001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arnaud/.local/lib/python3.6/site-packages/sklearn/feature_extraction/text.py:286: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", 'could', 'might', 'must', \"n't\", 'need', 'sha', 'wo', 'would'] not in stop_words.\n",
      "  sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score on 10 Cross Val with 10 folds : 0.7769999999999999\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizerPos = CountVectorizer()\n",
    "\n",
    "X = np.append(positives, negatives)\n",
    "y = np.append(np.ones(len(positives)), np.zeros(len(negatives)))\n",
    "Xtransorm = vectorizerPos.fit_transform(X)\n",
    "\n",
    "\n",
    "stop = set(stopwords.words('english'))\n",
    "vectorizerPos = TfidfVectorizer(stop_words = stop, tokenizer = word_tokenize, binary = True)\n",
    "Xtransorm = vectorizerPos.fit_transform(X)\n",
    "\n",
    "#TODO : tester une implÃ©mentation soit meme avec repeated k fold\n",
    "clf = MultinomialNB()\n",
    "cv = ShuffleSplit(n_splits=10, test_size=0.1, random_state=0)\n",
    "print('score on 10 Cross Val with 10 folds : ' + str(np.mean(cross_val_score(clf, Xtransorm, y, cv=cv))))  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stemming / Lemmatisation\n",
    "porter_stemmer = nltk.stem.porter.PorterStemmer()\n",
    "wordnet_lemmatizer = nltk.stem.wordnet.WordNetLemmatizer()\n",
    "\n",
    "#Tokenization of positives\n",
    "tokenizedPos = list(map(lambda com: word_tokenize(com), positives))\n",
    "stemsPos = np.array(tokenizedPos)\n",
    "lemmesPos = np.array(tokenizedPos)\n",
    "\n",
    "for i in range(len(positives)):\n",
    "    tokensStem = np.array([])\n",
    "    tokensLemm = np.array([])\n",
    "    for word in tokenizedPos[i]:\n",
    "        tokensStem = np.append(tokensStem, porter_stemmer.stem(word))\n",
    "        tokensLemm = np.append(tokensLemm, wordnet_lemmatizer.lemmatize(word))\n",
    "    stemsPos[i] = tokensStem\n",
    "    lemmesPos[i] = tokensLemm\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordCount = {}\n",
    "\n",
    "\n",
    "for comment in stemsPos:\n",
    "    for stem in comment:\n",
    "        try:\n",
    "            wordCount[stem] = wordCount[stem] + 1\n",
    "        except KeyError:\n",
    "            wordCount[stem] = 1 \n",
    "\n",
    "reductedStemPos = np.array(stemsPos)        \n",
    "\n",
    "for i in range(len(reductedStemPos)):\n",
    "    selectedStems = np.array([])\n",
    "    for stem in reductedStemPos[i]:\n",
    "        if (wordCount[stem] >= 1):\n",
    "            selectedStems = np.append(selectedStems, stem)\n",
    "    \n",
    "    reductedStemPos[i] = selectedStems            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/arnaud/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "stop = set(stopwords.words('english'))\n",
    "\n",
    "positive_no_stopwords = list(positives)\n",
    "\n",
    "for i in range(len(positives)):\n",
    "    positive_no_stopwords[i] = [word for word in word_tokenize(positives[i].lower()) if word not in stop]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import pos_tag\n",
    "\n",
    "positives_no_closedClass = list(positives)\n",
    "\n",
    "open_classes = {'FW', 'JJ', 'JRJ', 'JJS', 'NN', 'NNS', 'NNP', 'NNPS',\n",
    "                'RB', 'RBR', 'RBS', 'UH', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ'}\n",
    "\n",
    "for i in range(len(positives)):\n",
    "    tokens = word_tokenize(positives_no_closedClass[i].lower())\n",
    "    tags = pos_tag(tokens)\n",
    "    positives_no_closedClass[i] = list(filter(lambda tagged_word: tagged_word[1] in open_classes,tags))\n",
    "    positives_no_closedClass[i] = list(map(lambda x: x[0], positives_no_closedClass[i]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Word Presence #Normalement avec wordCount c'est bon\n",
    "#tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
